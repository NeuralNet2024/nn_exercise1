# -*- coding: utf-8 -*-
"""MulticlassFFNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GLcDybtflJsEqleMNCBUdySedq44mzk6
"""

import time
import warnings

import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.metrics import accuracy_score,\
 precision_score, recall_score, f1_score

from tensorflow import keras as ks

#PART 1a: DATA LOAD
#crate 2-D datasets to evaluate the models
n_samples = 420
numOfGroups = 2
inputData, outputData = datasets.make_circles(n_samples=n_samples, factor=.5,
                                       noise=.09)
#inputData, outputData = datasets.make_blobs(n_samples=n_samples,\
#                                       centers = numOfGroups)

# noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)

#PART 1b: DATA visualization
# demonstrate original groups distribution (up to five groups)
plotCharact = ['rx', 'b+', 'g.', 'y+', 'k^']
# plt.plot(inputData[outputData==0,0], inputData[outputData==0,1],\
#          'r.', label='Group1')
# plt.plot(inputData[outputData==1,0], inputData[outputData==1,1],\
#          'bx', label='Group2')
for groupIdx in range(0,numOfGroups):
  tmpGroupName = 'Group' + str(groupIdx+1)
  plt.plot(inputData[outputData==groupIdx,0],\
           inputData[outputData==groupIdx,1],\
           plotCharact[groupIdx], label = tmpGroupName)

plt.xlabel ('x1')
plt.ylabel ('x2')
plt.legend(loc='best')
plt.show()

#PART 2a: PREPROCESSING

#normalize the [input] data (if necessary)

# check output data

#PART 2b: TRAIN/TEST SETS CREATION
# create train/test sets
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(inputData, outputData,\
                                                    random_state=0)


plt.figure(figsize=(18,6))
ax1 = plt.subplot(121)
ax1.scatter(X_train[:,0], X_train[:,1], c=Y_train)

ax2 = plt.subplot(122, sharex=ax1)
ax2.scatter(X_test[:,0], X_test[:,1], c=Y_test)

plt.show()

#PART 3a: CREATE A MODEL TO USE
# define three (3) difrent FFNN architectures

#arch no 1: one hidden layer
def ffnet_no1(predifined_input_dim = 2,\
              predefined_activation_func = 'relu',\
              predifined_activation_map = 'softmax',\
              predifined_output_num = numOfGroups ):
  net = ks.models.Sequential()
  net.add(ks.layers.Dense(8,input_dim = predifined_input_dim,\
                          activation=predefined_activation_func))
  net.add(ks.layers.Dense(predifined_output_num,activation=\
                          predifined_activation_map))
  return net


#arch no 2: two hidden layers
def ffnet_no2(predifined_input_dim = 2,\
              predefined_activation_func = 'relu',\
              predifined_activation_map = 'softmax',\
              predifined_output_num = numOfGroups ):
  net = ks.models.Sequential()
  net.add(ks.layers.Dense(8,input_dim = predifined_input_dim,\
                          activation=predefined_activation_func))
  net.add(ks.layers.Dense(8,input_dim = predifined_input_dim,\
                          activation=predefined_activation_func))
  net.add(ks.layers.Dense(predifined_output_num,activation=\
                          predifined_activation_map))
  return net

#PART 3b: TRAIN THE MODEL [TRAIN/VAL SETS ONLy!]

#setup/load the net
#try 1: single output neuron

#setup, compile & train the model the model
# # try 1: single output neuron, MSE loss!
#CustomModel = ffnet_no1(predifined_output_num=1)
# CustomModel.compile(optimizer='adam', loss=ks.losses.MeanSquaredError(),\
#                     metrics=[ks.metrics.RootMeanSquaredError()])
# history = CustomModel.fit(X_train, Y_train,epochs = 50, batch_size=50,\
#                           validation_split = 0.1, verbose = 'auto')


#try 2: 2 outputs & Categorical cross entropy!
CustomModel = ffnet_no2()
CustomModel.compile(optimizer='adam', loss=ks.losses.CategoricalCrossentropy(),\
                    metrics=[ks.metrics.AUC()])

history = CustomModel.fit(X_train,\
                          ks.utils.to_categorical(Y_train),\
                          epochs = 50, batch_size=50,\
                          validation_split = 0.1, verbose = 'auto')

#PART 3c: check the performance [TRAIN/VAL SETS ONLy!]
#if unsure what history values you have
#history.history.keys()

#1st plot the history performance scores
plt.figure(figsize=(14,6))
plt.plot(history.history[list(history.history.keys())[0]])
plt.plot(history.history[list(history.history.keys())[2]])
plt.title('model root_mean_squared_error')
plt.ylabel(list(history.history.keys())[0])
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

#2nd check networks' [predicted] outputs against actual outputs.
Y_train_Predicted_SoftLabels = CustomModel.predict(X_train)


#adjust networks outcomes remember that we get continuous values
#it is also likely to have more than one outputs, we need the argmax
if len(Y_train_Predicted_SoftLabels.shape)>1:
  Y_train_Predicted = np.argmax(Y_train_Predicted_SoftLabels, axis=1)
else:
  Y_train_Predicted = np.rint(Y_train_Predicted_SoftLabels)



plt.figure(figsize=(14,6))
plt.plot( Y_train, '+', label='original')
plt.plot( Y_train_Predicted, 'x', label='predicted')
plt.title('Train set output values')
plt.xlabel('input values')
plt.ylabel('output values')
plt.legend(loc='best')
plt.show()  



print('F1 score for the train set is {:.2f}'.\
      format(f1_score(Y_train,Y_train_Predicted)))









